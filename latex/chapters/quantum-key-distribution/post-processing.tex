\section{Post-processing}

\Cref{fig:post_processing} introduces a generic post-processing procedure.
\begin{figure}[htb]
	\centering
	\includestandalone{figures/tikz/post-processing}
	\caption{Generic post-processing procedure for \gls{qkd}: First, the raw data from the quantum transmission is partitioned into data for calibration and data for key generation. The parameter estimation estimates an upper bound of Eve's information and the channel characteristics from the raw calibration data. If Eve's information exceeds a certain threshold, the protocol is aborted, or the current data frame is discarded. Symbol mapping, including basis sifting, transforms the raw key data to correlated key data. Information reconciliation corrects errors in the correlated key data and discards data blocks where error correction failed. Eve's information on the partially secret key is reduced to epsilon using privacy amplification. Finally, Alice and Bob verify that the post-processing was successful by comparing a hash of their secret key. If the hashes mismatch, the protocol is aborted, or the transmission block discarded.}\label{fig:post_processing}
\end{figure}

\FloatBarrier
\subsection{Symbol mapping}

The symbol mapping takes a sequence of symbols and maps it to a sequence of bits.
The two basis elements of qubit-based \gls{qkd} protocols have a natural interpretation as bits, i.e.,
\begin{equation}
	\begin{split}
		\vb{s}\colon\Omega&\to\{0,1\} \\
		\omega &\mapsto \vb{s}(\omega)
		=
		\begin{cases}
			1 & \text{"click"} \\
			0 & \text{"no click"}
		\end{cases}
		.
	\end{split}
\end{equation}
Not so the quadrature value in boson-based \gls{qkd} protocol.
While we can assign bits according to the sign of the quadrature
\begin{equation}
	\begin{split}
		\vb{s}\colon X\subseteq\mathbb{R}&\to\{0,1\} \\
		x &\mapsto \vb{s}(x)
		=
		\begin{cases}
			1 & x\geq0 \\
			0 & x<0
		\end{cases}
	\end{split}
	,
\end{equation}
such a symbol mapping turns out to be highly inefficient and more powerful techniques have been developed~\cite{VanAsche2004,Leverrier2008}.

Many \gls{cvqkd} protocols implement slice reconciliation~\cite{Grosshans2002} which includes a first error correction.
The idea of slice reconciliation is to partition the value range into $2^m$ bins of equal width and assign them a bit string according to the binary representation of their index number.
\Cref{fig:slice_reconciliation} illustrates slice reconciliation for $m=3$ yielding $2^m=8$ different bins for a standard normal distributed random variable.
Alice's $x_i$ and Bob's variables $x_i^\prime$ are not equal but correlated.
Alice and Bob assign bits to their variable according to the binning scheme.
If Alice and Bob are lucky, the assignment yields the same bit sequence.
For example, Alice and Bob both assign $x_1$ and $x_1^\prime$ to the third bin and now share bit string $011$.
However, it happens that Alice and Bob assign different bins, see, for instance, $x_2$ and $x_2^\prime$ which are assigned to the fourth bin (Alice) and the fifth bin (Bob).
In this particular case, if Bob knows the least significant bit from Alice, he is able to correct his bit string assignment to $100$.
\begin{figure}[htb]
	\centering
	\includestandalone{figures/pgfplots/slice-reconciliation}
	\caption{Slice reconciliation for $m=3$ with $2^m=8$ bins for samples from a standard normal distribution. Alice prepared the values $x_1,x_2$ and Bob measured the values $x_1^\prime,x_2^\prime$. $x_1$ and $x_1^\prime$ are located in the same bin $3$ but $x_2$ and $x_2^\prime$ are located in bin $4$ respectively bin $5$. Slice reconciliation is able to detect and correct such errors with high probability.}\label{fig:slice_reconciliation}
\end{figure}

In an alternative scheme, known as multidimensional reconciliation~\cite{Leverrier2008}, Alice and Bob combine multiple variables into a $d$-dimensional vector, e.g., $\vb{x}=(x_1,\dots,x_d)$ respectively $\vb{x}^\prime=(x_1^\prime,\dots,x_d^\prime)$ and agree on a set of equally-spaced apart symbol vectors $\vu{v}_1,\dots,\vu{v}_m\in\mathbb{R}^d$.
Now, Bob finds a rotation matrix $R^\prime\in\mathbb{R}^{d\times d}$ which maps his variable vector onto a symbol vector $\vb{v}_j=R^\prime\vb{x}^\prime$.
Bob then discloses the parameters of his rotation matrix $R^\prime$ to Alice.
Alice rotates her variable vector $R\vb{x}$ and finds the symbol vector $\vb{v}_k$ most with least distance to $R\vb{x}$.
\begin{figure}[htb]
	\centering
	\includestandalone{figures/pgfplots/multidimensional-reconciliation}
	\caption{Multidimensional reconciliation in $\mathbb{R}^2$: Alice and Bob encoded their prepared and measured values into two-dimensional vectors $\vb{x}$ and $\vb{x}^\prime$. Bob calculates the rotation matrix $R$ that rotates $\vb{x}^\prime$ close to the symbol vector $\vu{v}_4$ and shares the rotational parameters with Alice. Alice applies the rotation to her vector $R\vb{x}$. Both rotated vectors of Alice and Bob $R\vb{x}$ and $R\vb{x}^\prime$ are assigned the $11$ bit string with high confidence.}\label{fig:multidimensional_reconciliation}
\end{figure}
Multidimensional reconciliation works best in high-dimensional spaces as the norm of random vector approaches one (unit sphere) for $n\to\infty$.

\FloatBarrier
\subsection{Information reconciliation}

Information reconciliation summarizes methods required for Alice and Bob to agree on shared data.
It includes error correction, and discarding of data failed to correct.

Let us first consider procedures for error correction.
Error correction is a subdiscipline of coding theory, or more precisely, channel coding, which studies the arrangement of data for efficient and reliable transmission, see \Cref{fig:error_correction_codes}.
The following discussion is a very brief introduction to binary linear codes based of Ref.~\cite{MacKay2003,Mildenberger2013}.
\begin{figure}[htb]
	\centering
	\includestandalone{figures/tikz/error-correction-codes}
	\caption{Taxonomy of codes in coding theory with emphasis on linear block codes for error correction: The linear block codes are distinguished by the constraints on their generator matrix.}\label{fig:error_correction_codes}
\end{figure}
A $(n,k)$ binary linear code encodes $k$-bit messagewords into $n$-bit codewords.
The additional $n-k$ check bits are used to detect and correct errors, e.g., bit flips.
In general, it is impossible to correct for all errors although practical linear block codes closely approach the theoretical (Shannon) limit set by the noisy-channel coding theorem.

Let $\vb{m}\in\{0,1\}^{1\times k}$ be a messageword, then the generator matrix
\begin{equation}
	G
	=
	\begin{pmatrix}[c|c]
		I_k & P
	\end{pmatrix}
	=
	\begin{pmatrix}[cccc|cccc]
		1 & 0 & \cdots & 0 & p_{1,1} & p_{1,2} & \cdots & p_{1,m} \\
		0 & 1 & \cdots & 0 & p_{2,1} & p_{2,2} & \cdots & p_{2,m}\\
		\vdots & \vdots  & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & 1 & p_{n-m,1} & p_{n-m,2} & \cdots & p_{n-m,m} \\
	\end{pmatrix}
	\in\{0,1\}^{k\times n}
	,
\end{equation}
wherein $I_k\in\{0,1\}^{k\times k}$ denotes an identity matrix and $P\in\{0,1\}^{k\times(n-k)}$ denotes the (parity) check matrix,
encodes the messageword $\vb{m}$ into the codeword
\begin{equation}
	\vb{x}
	=
	\vb{m}G
	\in\{0,1\}^{1\times n}
\end{equation}
with the matrix multiplication being defined on the binary field $\mathbb{F}_2$\footnote{Alternatively, we can define the addition and multiplication on the real field with modulo two.}.
The explicit form of the generator matrix depends on the linear block code.
For instance, the $(n,1)$ repetition code has the generator matrix
\begin{equation}
	G
	=
	\begin{pmatrix}
		1 & 1 & \cdots & 1
	\end{pmatrix}
	\in\{0,1\}^{1\times n}
\end{equation}
and the $(7,4)$ Hamming code has the generator matrix
\begin{equation}
	G
	=
	\begin{pmatrix}
		1 & 0 & 0 & 0 & 1 & 1 & 0 \\
		0 & 1 & 0 & 0 & 1 & 0 & 1 \\
		0 & 0 & 1 & 0 & 0 & 1 & 1 \\
		0 & 0 & 0 & 1 & 1 & 1 & 1 \\
	\end{pmatrix}
	\in\{0,1\}^{4\times 7}
	.
\end{equation}
For the post-processing, we use \gls{ldpc}~\cite{Gallager1962} where the generator matrix is a sparse random matrix.
\Cref{tab:repetition_codewords} and \Cref{tab:hamming_codewords} show the possible codewords for the $(3,1)$ repetition and $(7,4)$ Hamming code.
From \Cref{tab:repetition_codewords}, we note that the repetition code repeats the message word $n-k$ times.
The $(n,1)$ repetition code is able to detect all bit errors except the error when all bits are flipped and correct up to $\floor{(n-1)/2}$ bit errors~\cite[p.~5]{MacKay2003}.
\begin{table}[htb]
	\centering
	\begin{tabular}{c|c|cc}
		\toprule
		Nr. & Information & \multicolumn{2}{c}{Check} \\
		\midrule
			1 & 0 & 0 & 0 \\
			2 & 1 & 1 & 1 \\
		\bottomrule
	\end{tabular}
	\caption{Possible codewords for $(3,1)$ repetition code.}\label{tab:repetition_codewords}
\end{table}
The $(7,4)$ Hamming code is a more efficient block code which uses parity checks to detect and correct single-bit errors~\cite[p.~10]{MacKay2003}.
\begin{table}[htb]
	\centering
	\begin{tabular}{c|cccc|ccc}
		\toprule
		Nr. & \multicolumn{4}{c}{Information} & \multicolumn{3}{c}{Check} \\
		\midrule
			1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
			2 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
			3 & 0 & 0 & 1 & 0 & 1 & 1 & 0 \\
			4 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
			5 & 0 & 1 & 0 & 0 & 1 & 0 & 1 \\
			6 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\
			7 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\
			8 & 0 & 1 & 1 & 1 & 1 & 0 & 0 \\
			9 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\
			10 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
			11 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
			12 & 1 & 0 & 1 & 1 & 0 & 1 & 0 \\
			13 & 1 & 1 & 0 & 0 & 1 & 1 & 0 \\
			14 & 1 & 1 & 0 & 1 & 0 & 0 & 1 \\
			15 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
			16 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
		\bottomrule
	\end{tabular}
	\caption{Possible codewords for $(7,4)$ Hamming code~\cite[p.~109]{Mildenberger2013}.}\label{tab:hamming_codewords}
\end{table}
The received codeword $\vb{y}$ of a linear channel equals the sent codeword $\vb{x}$ plus a noise (row) vector $\vb{n}$, i.e.,
\begin{equation}
	\vb{y}
	=
	\vb{x}
	+
	\vb{n}
	\in\{0,1\}^n
	.
\end{equation}
The noise vector introduces bit flips according to an assumed channel model, for example, the binary symmetric channel where a single bit flip occurs with probability $p$, see, e.g., Ref.~\cite[p.~148]{MacKay2003}.
To detect errors of a received codeword $\vb{y}$, one uses the parity-check matrix
\begin{equation}
	H
	=
	\begin{pmatrix}[c|c]
		-P^\trans & I_{n-k}
	\end{pmatrix}
	=
	\begin{pmatrix}
		p_{1,1} & p_{2,1} & \cdots & p_{n-m,1} & 1 & 0 & \cdots & 0 \\
		p_{1,2} & p_{2,2} & \cdots & p_{n-m,2} & 0 & 1 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		p_{1,m} & p_{2,m} & \cdots & p_{n-m,m} & 0 & 0 & \cdots & 1 \\
	\end{pmatrix}
	\in\{0,1\}^{(n-k)\times n}
\end{equation}
where we used $-p_{i,j}=p_{i,j}$ for elements of the binary field $p_{i,j}\in\mathbb{F}_2$.
The parity-check matrix is orthogonal to the generator matrix~\cite[p.~95]{Mildenberger2013}, i.e.,
\begin{equation}
	GH^\trans
	=
	\vb{0}
	=
	HG^\trans
	.
\end{equation}
The orthogonality between generator and parity-check matrix implies that for the received codeword $\vb{y}$, the parity-check matrix yields, a binary vector called the syndrome (column) vector
\begin{equation}
	\vb{s}
	=
	H\vb{y}^\trans
	=
	HG^\trans\vb{m}^\trans
	+
	H\vb{n}^\trans
	=
	H\vb{n}^\trans
\end{equation}
which only depends on the noise (row) vector $\vb{n}$.
If the block code does not detect any error, we have $\vb{s}=0$.
\begin{table}[htb]
	\centering
	\begin{tabular}{c|cccc|ccc}
		\toprule
		\multirow{2}{*}{Nr.} & \multicolumn{4}{c}{Received} & \multicolumn{3}{c}{Syn-} \\
		& \multicolumn{4}{c}{codeword} & \multicolumn{3}{c}{drome} \\
		\midrule
			1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
			2 & 0 & 0 & 0 & 1 & 0 & 0 & 1 \\
			3 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
			4 & 0 & 0 & 1 & 1 & 0 & 1 & 1 \\
			5 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
			6 & 0 & 1 & 0 & 1 & 1 & 0 & 1 \\
			7 & 0 & 1 & 1 & 0 & 1 & 1 & 0 \\
			8 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \\
			9 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\
			10 & 1 & 0 & 0 & 1 & 1 & 1 & 0 \\
			11 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
			12 & 1 & 0 & 1 & 1 & 1 & 0 & 0 \\
			13 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
			14 & 1 & 1 & 0 & 1 & 0 & 1 & 0 \\
			15 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\
			16 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
		\bottomrule
	\end{tabular}
	\caption{Possible syndromes for the $(3,1)$ repetition code: The first and last row are correct codewords (without noise) and yield a zero syndrome indicating no error. All other received codewords contain bit flips and thereby non-zero syndromes.}\label{tab:repetition_syndromes}
\end{table}
\Cref{tab:repetition_syndromes} lists the possible syndromes for the $(3,1)$ repetition code.
To correct the error, one looks up the calculated syndrome in an error correction table.
For example, \Cref{tab:hamming_correction} lists the bit-error corrections assigned to each syndrome of the $(7,4)$ Hamming code.
\begin{table}[htb]
	\centering
	\begin{tabular}{cccccccc}
		\toprule
		Syndrome $\vb{s}$ & $001$ & $010$ & $011$ & $100$ & $101$ & $110$ & $111$ \\
		Unflip bit & $y_7$ & $y_6$ & $y_4$ & $y_5$ & $y_1$ & $y_2$ & $y_3$ \\
		\bottomrule
	\end{tabular}
	\caption{Bit-error correction lookup table for the $(7,4)$ Hamming code~\cite[p.~11]{MacKay2003}.}\label{tab:hamming_correction}
\end{table}

So far, we have implicitly assumed the (parity) check bits to be transmitted together with the data bits such that errors can be directly detected and corrected (forward error correction).
Forward error correction does not make sense for \gls{qkd} as the data bits are a result of the post-processing and not directly transmitted.
Instead of exchanging the check bits together with a data index as part of the error correction in \gls{qkd}, one instead directly calculates the syndromes and transmits these depending on the implementation details.

There cannot exist a perfect error correction protocol as it is always possible that bits are flipped such that a different valid codeword is received.
However, we have no use of data blocks where error correction have failed and we can simply discard them.
To identify these data blocks, Alice and Bob exchange hashes of their data blocks to verify success of the error correction.

\FloatBarrier
\subsection{Privacy amplification}

The final step of most \gls{qkd} protocols is privacy amplification which removes Eve's information from the key.
More formally, let us assume a key of length $n$, $s\in\{0,1\}^n$, and that Eve has partial information over the key equivalent to $k$ bits.
For privacy amplification, Alice and Bob need to agree on a map $f\colon\{0,1\}^n\to\{0,1\}^r$ with $r\leq n-k$ which extracts Eve's information from the key.

Bennett and Brassard proposed privacy amplification four years after BB84 in 1988.
An information-secure proof that privacy amplification is possible was published one year later, now known as the leftover-hash-lemma~\cite{Impagliazzo1989} and later extended to the quantum leftover-hash-lemma~\cite{Renner2005}.
In 1995, Bennett and Brassard generalized the privacy amplification outside of \gls{qkd}~\cite{Bennett1995}.

In practice, privacy amplification is performed by randomly XOR the bits of a key.
Let us illustrate this using a partially secret key $(s_1,s_2,s_3)\in\{0,1\}^3$ where Eve knows the value of $s_3$.
After privacy amplification the initial key could for example $(s_1\oplus s_3,s_2\oplus s_3)$ or $(s_1\oplus s_2,s_2\oplus s_3)$ and Eve's knowledge of the key bit $s_3$ does not help her infering a bit of the new secret key.

Although not directly related to privacy amplification, it is helpful to consider the following Gedankenexperiment to gain intuition about XORing:
Let us assume Alice and Bob generate a key by flipping a coin $n$ times and assigning heads to zero and tails to one.
Eve having no information about the key corresponds to Alice and Bob using an unbiased coin yielding heads and tails with equal probability.
Eve having full information about the key corresponds to Alice and Bob using a biased coin always yielding, e.g., heads.
Therefore, we can use the probability of the coin yielding heads, $p\in[0,1]$, to indicate Eve's information.
If we now start to XOR all outcomes, we see the probability for obtaining either a one or zero to approach $1/2$.
More precisely, let $X_1,X_2,\dots,X_N$ be \gls{iid} Bernoulli random variables with probability $p$, $X_i\sim\text{Bern}(p)$, then the probability that XORing of all outcomes equals one is
\begin{equation}
	\mathbb{P}\left[
		\text{"XOR of outcomes equals one"}
	\right]
	=
	\mathbb{P}\left[
		\sum_{n=1}^NX_n\bmod2
		=
		1
	\right]
	.
\end{equation}
The sum of \gls{iid} Bernoulli random variables $X_1,\dots,X_n$ equals a Binomial random variable $Y=\sum_{n=1}^NX_n\sim\text{Binom}(N,p)$, yielding
\begin{equation}
	\mathbb{P}\left[
		Y\bmod2
		=
		1
	\right]
	=
	\sum^N_{k\text{ odd}}
	\mathbb{P}\left[Y=k\right]
	=
	\sum^N_{k\text{ odd}}
	\binom{N}{k}
	p^k(1-p)^{N-k}
	\label{eq:xoring_prop}
	.
\end{equation}
Using the identity from Ref.~\cite{stackexchange2528974}, we can simplify \cref{eq:xoring_prop} to
\begin{equation}
	\mathbb{P}\left[
		Y\bmod2
		=
		1
	\right]
	=
	\frac{1}{2}
	\left(
		1-(2p-1)^N
	\right)
	\xrightarrow{N\to\infty}
	\frac{1}{2}
\end{equation}
where limit is true for $p\in]0,1[$ so even for $p=0.9999$.
Moreover, for any $\varepsilon>0$ we can choose $N$ such that $\mathbb{P}\left[Y\bmod2=1\right]-1/2<\varepsilon$, meaning that we can arbitrarily reduce Eve's information by XORing more coin flips.

For practical applications, privacy amplification operates on the class of linear maps,
\begin{equation}
	\begin{split}
		f\colon\{0,1\}^n&\to \{0,1\}^{n-k}\\
		\vb{s}&\mapsto M\vb{s}\bmod{2}
	\end{split}
\end{equation}
where $M\in\{0,1\}^{r\times n}$ is a random boolean matrix with at least $k+1$ ones~\cite{Bennett1985}.
For construction of $M$ it is convenient to use Toeplitz matrices~\cite[p.~11]{Fung2010}.
A $n\times m$ Toeplitz matrix $A$ has the form
\begin{equation}
	A
	=
	\begin{pmatrix}
		a_{i,j}
	\end{pmatrix}
	=
	\begin{pmatrix}
		a_{i-j}
	\end{pmatrix}
	=
	\begin{pmatrix}
		a_0 & a_{-1} & \cdots & a_{1-m} \\
		a_1 & a_0 & \cdots & a_{2-m} \\
		\vdots & \ddots & \cdots & \vdots \\
		a_{n-1} & a_{n-2} & \cdots & a_{n-m} \\
	\end{pmatrix}
	.
\end{equation}
For example, if Alice and Bob estimate Eve to know three bits, they could agree on the Toeplitz coefficients $(a_{-4},a_{-3},\dots,a_1,a_2)=(0,1,1,1,0,1,0)$ yielding the mapping
\begin{equation}
	f(\vb{s})
	=
	\begin{pmatrix}
		0 & 1 & 1 & 1 & 0 \\
		1 & 0 & 1 & 1 & 1 \\
		0 & 1 & 0 & 1 & 1 \\
	\end{pmatrix}
	\begin{pmatrix}
		0 \\
		1 \\
		1 \\
		0 \\
		1 \\
	\end{pmatrix}
	\bmod{2}
\end{equation}
which for the key $\vb{s}=(0,1,1,0,1)$ would yield the secret ket $\vb{s}^\prime=(0,0,0)$.
